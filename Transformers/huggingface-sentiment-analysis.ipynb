{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HuggingFace for Sentiment Analysis: An Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "For this tutorial, I'll be using hugging face to perform sentiment analysis. I will be checking out and exploring the model hub and then fine-tune the chosen model to boost performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needeed modules\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task for the pipeline is provided, in this case we use the \n",
    "'sentiment-analysis' task which will return a text classification class. \n",
    "\n",
    "Sentiment analysis is a form of text classification as well as documented\n",
    "on the huggingface website: [huggingface.co/transformers/main_classes/pipelines.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "# Specifying the sentiment analysis task to be performed with the pipeline\n",
    "sentiment_classifier_1 = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets try some samples!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9808171987533569}]\n"
     ]
    }
   ],
   "source": [
    "test_1 = sentiment_classifier_1 (\"Oh, if it isn't the thief of the century.\")\n",
    "print(test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSIGHT:**\n",
    "As you can see the result of the text shows a dictionary with a label and a score as keys as well as their values. Which shows that the sentence is negative with a score of 0.9808171987533569."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's use the classifier for a list of sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'NEGATIVE', 'score': 0.9774008393287659}\n",
      "{'label': 'NEGATIVE', 'score': 0.9614507555961609}\n",
      "{'label': 'POSITIVE', 'score': 0.9981259703636169}\n"
     ]
    }
   ],
   "source": [
    "test_2 = sentiment_classifier_1 ([\n",
    "    \"Oh, if it isn't the thief of the century\", \n",
    "    \"Fly! You fools!\", \n",
    "    \"I can do this all day\"\n",
    "])\n",
    "\n",
    "# Printing the result for each sentence\n",
    "for res in test_2:\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a specific model and tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model (model name) was chosen from the model hub of huggingface\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'NEGATIVE', 'score': 0.9774008393287659}\n",
      "{'label': 'NEGATIVE', 'score': 0.9614507555961609}\n",
      "{'label': 'POSITIVE', 'score': 0.9981259703636169}\n"
     ]
    }
   ],
   "source": [
    "# Specifying the model name for the classifier\n",
    "\n",
    "sentiment_classifier_2 = pipeline(\"sentiment-analysis\", model = model_name)\n",
    "\n",
    "# Testing the new classifer on the list of sentences (test_2)\n",
    "test_2 = sentiment_classifier_2 ([\n",
    "    \"Oh, if it isn't the thief of the century\", \n",
    "    \"Fly! You fools!\", \n",
    "    \"I can do this all day\"\n",
    "])\n",
    "\n",
    "for res in test_2:\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A different approach to specifying a model and tokeniser**\n",
    "\n",
    "This just a generic class for a tokeniser and also a sequence classification which lends itself to giving you some more functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'NEGATIVE', 'score': 0.9774008393287659}\n",
      "{'label': 'NEGATIVE', 'score': 0.9614507555961609}\n",
      "{'label': 'POSITIVE', 'score': 0.9981259703636169}\n"
     ]
    }
   ],
   "source": [
    "# Using the specified model and tokeniser\n",
    "sentiment_classifier_3 = pipeline(\"sentiment-analysis\", model = model, tokenizer = tokeniser)\n",
    "\n",
    "#testing this classifier on the list of examples\n",
    "test_2 = sentiment_classifier_3 ([\n",
    "    \"Oh, if it isn't the thief of the century\", \n",
    "    \"Fly! You fools!\", \n",
    "    \"I can do this all day\"\n",
    "])\n",
    "\n",
    "for res in test_2:\n",
    "    print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring the tokeniser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['oh', ',', 'if', 'it', 'isn', \"'\", 't', 'the', 'thief', 'of', 'the', 'century']\n",
      "Tokens IDs: [2821, 1010, 2065, 2009, 3475, 1005, 1056, 1996, 12383, 1997, 1996, 2301]\n",
      "Input IDs: {'input_ids': [101, 2821, 1010, 2065, 2009, 3475, 1005, 1056, 1996, 12383, 1997, 1996, 2301, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokeniser.tokenize(\"Oh, if it isn't the thief of the century\")\n",
    "token_ids = tokeniser.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokeniser(\"Oh, if it isn't the thief of the century\")\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Tokens IDs: {token_ids}\")\n",
    "print(f\"Input IDs: {input_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSIGHTS:** \n",
    "\n",
    "We get a list of the tokens in the sentence. Which is basically a list of the words in the sentence. **Note:** The punctuations in the sentence will also be given tokens. \n",
    "\n",
    "The tokens or token ids are basically the mathematical representation of the words or tokens which the model can work with/understand.\n",
    "\n",
    "When you look at the input ids, you'll find that the tokens are similar to the token ids, however, there is a _101_ and _102_ at the beggining and end of the input ids respectfully. This is just the beggining of string and end of string tokens.\n",
    "\n",
    "The input ids are what can be passed to the model to do the prediction manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', ',', 'if', 'it', 'isn', \"'\", 't', 'the', 'thief', 'of', 'the', 'century']\n"
     ]
    }
   ],
   "source": [
    "# Checking how the convert_ids_to_tokens work\n",
    "rev_tokens = tokeniser.convert_ids_to_tokens(token_ids)\n",
    "print(rev_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets try some manual predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Providing the training set\n",
    "X_train = [\n",
    "    \"Oh, if it isn't the thief of the century\", \n",
    "    \"Fly! You fools!\", \n",
    "    \"I can do this all day\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2821,  1010,  2065,  2009,  3475,  1005,  1056,  1996, 12383,\n",
      "          1997,  1996,  2301,   102],\n",
      "        [  101,  4875,   999,  2017, 18656,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1045,  2064,  2079,  2023,  2035,  2154,   102,     0,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batch = tokeniser(\n",
    "    X_train, padding = True, \n",
    "    truncation = True, \n",
    "    max_length = 512, \n",
    "    return_tensors = \"pt\"\n",
    "    )\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.0213), logits=tensor([[ 2.1371, -1.6299],\n",
      "        [ 1.7681, -1.4484],\n",
      "        [-3.0584,  3.2194]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.9774, 0.0226],\n",
      "        [0.9615, 0.0385],\n",
      "        [0.0019, 0.9981]])\n",
      "tensor([0, 0, 1])\n",
      "['NEGATIVE', 'NEGATIVE', 'POSITIVE']\n"
     ]
    }
   ],
   "source": [
    "# Passing the output into our model manually\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch, labels = torch.tensor([0, 0, 1]))\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim = 1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim = 1)\n",
    "    print(labels)\n",
    "    labels = [model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "60de91654a7d3bbcd8f93c3fd8aff464fce5c71470dd94c600ee6e33f99551d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
