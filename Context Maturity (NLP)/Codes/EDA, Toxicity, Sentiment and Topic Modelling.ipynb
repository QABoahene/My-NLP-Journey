{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Exploratory Data Analysis, Sentiment Analysis and Topic Modelling\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing modules and data\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import datetime as dt  \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 15\n",
    "width = 0.75\n",
    "sns.set_palette(sns.color_palette('tab20', 20))\n",
    "import plotly.graph_objs as go\n",
    "from datetime import date, timedelta\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from detoxify import Detoxify\n",
    "import chart_studio.plotly as py\n",
    "from plotly.offline import iplot\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "data = pd.read_csv('/Users/qab/Desktop/Personal/NLP Projects/Context Maturity (NLP)/Data/jon_bellion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "source": [
    "## Drawing Empath Themes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the empath themes in the lyrics\n",
    "def extract_empath(lyrics):\n",
    "    return lexicon.analyze(lyrics)\n",
    "\n",
    "#Creates tags with the empath themes based on score\n",
    "def make_tags(tags):\n",
    "    tgs = [k for k, v in tags.items() if v != 0] #Helps set limit on tags to be kept\n",
    "    #tgs = sorted(tags.items(), key = lambda x: x[1], reverse = True)\n",
    "    return tgs\n",
    "\n",
    "#Processes the dictionary of tags and keeps the keys\n",
    "def process(st):\n",
    "    st = str(st)\n",
    "    table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    new_s = st.translate(table)\n",
    "    return new_s\n",
    "\n",
    "data['empath_themes'] = data['lyrics'].apply(extract_empath).apply(make_tags).apply(process).apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of songs per album\n",
    "data.groupby('album').count()['titles'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Number of songs', \n",
    "    linecolor='black', \n",
    "    opacity=0,\n",
    "    title='Bar chart of songs per album release', \n",
    "    xTitle='Albums'\n",
    "    )"
   ]
  },
  {
   "source": [
    "## Word count of lyrics before and after text preprocessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count\n",
    "word_count = lambda x: len(str(x).split())\n",
    "data['song_length'] = data['lyrics'].astype(str).apply(len)\n",
    "data['lyrics_word_count'] = data['lyrics'].apply(word_count)\n",
    "data['processed_lyrics_word_count'] = data['processed_lyrics'].apply(word_count)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all songs per album for unprocessed lyrics\n",
    "sns.set(rc = {'figure.figsize':(15, 10)})\n",
    "album1 = data[data['album'] == 'translation_through_speakers']['lyrics'].str.len()\n",
    "sns.distplot(album1, label = 'Translation Through Speakers')\n",
    "album2 = data[data['album'] == 'the_separation']['lyrics'].str.len()\n",
    "sns.distplot(album2, label = 'The Separation')\n",
    "album3 = data[data['album'] == 'the_definition']['lyrics'].str.len()\n",
    "sns.distplot(album3, label = 'The Definition')\n",
    "album4 = data[data['album'] == 'the_human_condition']['lyrics'].str.len()\n",
    "sns.distplot(album4, label = 'The Human Condition')\n",
    "album5 = data[data['album'] == 'glory_sound_prep']['lyrics'].str.len()\n",
    "sns.distplot(album5, label = 'Glory Sound Prep')\n",
    "plt.title('Length of lyrics per album released (Lyrics Not Processed)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of all songs per album for processed lyrics\n",
    "sns.set(rc = {'figure.figsize':(15, 10)})\n",
    "album1 = data[data['album'] == 'translation_through_speakers']['processed_lyrics'].str.len()\n",
    "sns.distplot(album1, label = 'Translation Through Speakers')\n",
    "album2 = data[data['album'] == 'the_separation']['processed_lyrics'].str.len()\n",
    "sns.distplot(album2, label = 'The Separation')\n",
    "album3 = data[data['album'] == 'the_definition']['processed_lyrics'].str.len()\n",
    "sns.distplot(album3, label = 'The Definition')\n",
    "album4 = data[data['album'] == 'the_human_condition']['processed_lyrics'].str.len()\n",
    "sns.distplot(album4, label = 'The Human Condition')\n",
    "album5 = data[data['album'] == 'glory_sound_prep']['processed_lyrics'].str.len()\n",
    "sns.distplot(album5, label = 'Glory Sound Prep')\n",
    "plt.title('Length of lyrics per album released (Pre-Processed Lyrics)')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of songs and their length\n",
    "data['song_length'].iplot(\n",
    "    kind = 'hist',\n",
    "    bins = 56, #A small database so I limited this to the number of rows so the spread will be even. \n",
    "    xTitle = 'Song Length',\n",
    "    linecolor = 'black',\n",
    "    yTitle = 'Number of Songs',\n",
    "    title = 'Song Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count of lyrics before and after cleaning\n",
    "data[['lyrics_word_count', 'processed_lyrics_word_count']].iplot(\n",
    "    kind = 'hist',\n",
    "    bins = 20, #A small database so I limited this to the number of rows so the spread will be even. \n",
    "    xTitle = 'Word Count',\n",
    "    linecolor = 'black',\n",
    "    yTitle = 'Number of Songs',\n",
    "    title = 'Lyrics Count Per Song Distribution Before and After Pre-processing')"
   ]
  },
  {
   "source": [
    "## Sentiment, Toxicity and Subjectivity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting polarity and subjectivity with Textblob\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "#Creating new features for polarity and subjectivity\n",
    "data['polarity'] = data['lyrics'].apply(pol)\n",
    "data['subjectivity'] = data['lyrics'].apply(sub)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "source": [
    "## Toxicity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_dict = []\n",
    "for lyric in data['lyrics']:\n",
    "    toxicity = Detoxify('original').predict(lyric)\n",
    "    toxicity_dict.append(toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxicity_dict\n",
    "toxicity_df = pd.DataFrame(toxicity_dict)\n",
    "#toxicity_df\n",
    "data = pd.concat([data, toxicity_df], axis = 1)\n",
    "#data.to_csv('Jon Bellion Discography DS Metadata.csv')\n",
    "data.head()"
   ]
  },
  {
   "source": [
    "## Visualising the sentiment and toxicity of songs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment polarity distribution - shows polarity range and number of songs in that range\n",
    "data['polarity'].iplot(\n",
    "    kind = 'hist',\n",
    "    bins = 56, #A small database so I limited this to the number of rows so the spread will be even. \n",
    "    xTitle = 'Lyrics Polarity',\n",
    "    linecolor = 'black',\n",
    "    yTitle = 'Number of Songs',\n",
    "    title = 'Sentiment Polarity Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subjectivity distribution - shows subjectivity range and number of songs in that range\n",
    "data['toxicity'].iplot(\n",
    "    kind = 'hist',\n",
    "    bins = 56, #A small database so I limited this to the number of rows so the spread will be even. \n",
    "    xTitle = 'Lyrics Toxicity',\n",
    "    linecolor = 'black',\n",
    "    yTitle = 'Number of Songs',\n",
    "    title = 'Toxiciy Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D density jointplot comparing sentiment and the toxicity of the songs\n",
    "trace1 = go.Scatter(\n",
    "    x=data['polarity'], y=data['toxicity'], mode='markers', name='points',\n",
    "    marker=dict(color='rgb(102,0,0)', size=2, opacity=0.4)\n",
    ")\n",
    "trace2 = go.Histogram2dContour(\n",
    "    x=data['polarity'], y=data['toxicity'], name='density', ncontours=20,\n",
    "    colorscale='Hot', reversescale=True, showscale=False\n",
    ")\n",
    "trace3 = go.Histogram(\n",
    "    x=data['polarity'], name='Sentiment polarity density',\n",
    "    marker=dict(color='rgb(102,0,0)'),\n",
    "    yaxis='y2'\n",
    ")\n",
    "trace4 = go.Histogram(\n",
    "    y=data['toxicity'], name='Song Toxicity density', marker=dict(color='rgb(102,0,0)'),\n",
    "    xaxis='x2'\n",
    ")\n",
    "plot_data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "layout = go.Layout(\n",
    "    showlegend=False,\n",
    "    autosize=False,\n",
    "    width=600,\n",
    "    height=550,\n",
    "    xaxis=dict(\n",
    "        domain=[0, 0.85],\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        domain=[0, 0.85],\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    margin=dict(\n",
    "        t=50\n",
    "    ),\n",
    "    hovermode='closest',\n",
    "    bargap=0,\n",
    "    xaxis2=dict(\n",
    "        domain=[0.85, 1],\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        domain=[0.85, 1],\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=plot_data, layout=layout)\n",
    "iplot(fig, filename='2dhistogram-2d-density-plot-subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "data, \n",
    "x = \"polarity\", \n",
    "y = \"toxicity\",\n",
    "labels = {'polarity': 'Polarity',\n",
    "'toxicity': 'Toxicity', \n",
    "'album': 'Albums'},\n",
    "hover_name = 'titles',\n",
    "title = 'Polarity and Toxicity Plot',\n",
    "width=1000,\n",
    "height=800,\n",
    "color = 'album'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "## Drill down analysis of sentiment throughout the progression of the song"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to split lyrics into 'n' number of chunks\n",
    "def split_text(text, n = 5):\n",
    "    '''Takes in a string of text(lyrics) and splits into n equal parts, with a default of 10 equal parts.'''\n",
    "    \n",
    "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
    "    length = len(text)\n",
    "    size = math.floor(length / n)\n",
    "    start = np.arange(0, length, size)\n",
    "    \n",
    "    # Pull out equally sized pieces of text and put it into a list\n",
    "    split_list = []\n",
    "    for piece in range(n):\n",
    "        split_list.append(text[start[piece]:start[piece]+size])\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying it out\n",
    "list_pieces = []\n",
    "for t in data.processed_lyrics:\n",
    "    split = split_text(t)\n",
    "    list_pieces.append(split)\n",
    "    \n",
    "#list_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the polarity for the pieces of lyric chunks\n",
    "polarity_lyrics = []\n",
    "for lp in list_pieces:\n",
    "    polarity_piece = []\n",
    "    for p in lp:\n",
    "        polarity_piece.append(TextBlob(p).sentiment.polarity)\n",
    "    polarity_lyrics.append(polarity_piece)\n",
    "    \n",
    "#polarity_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting sentment changes for the chunck of texts (lyrics)\n",
    "plt.plot(polarity_lyrics[1])\n",
    "plt.title(data['titles'].index[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for all songs\n",
    "plt.rcParams['figure.figsize'] = [50, 40]\n",
    "\n",
    "for index, title in enumerate(data.index):    \n",
    "    plt.subplot(8, 7, index+1)\n",
    "    plt.plot(polarity_lyrics[index])\n",
    "    plt.plot(np.arange(0, 5), np.zeros(5))\n",
    "    plt.title(data['titles'][index], fontsize = 18)\n",
    "    plt.ylim(ymin=-1, ymax=1)\n",
    "    plt.xlim(xmin=0, xmax=4)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boxplot showcasing the sentiment range for the albums\n",
    "y0 = data.loc[data['album'] == 'translation_through_speakers']['polarity']\n",
    "y1 = data.loc[data['album'] == 'the_separation']['polarity']\n",
    "y2 = data.loc[data['album'] == 'the_definition']['polarity']\n",
    "y3 = data.loc[data['album'] == 'the_human_condition']['polarity']\n",
    "y4 = data.loc[data['album'] == 'glory_sound_prep']['polarity']\n",
    "\n",
    "trace0 = go.Box(\n",
    "    y = y0,\n",
    "    name = 'Translation Through Speakers',\n",
    "    marker = dict(\n",
    "        color = 'rgb(214, 12, 140)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace1 = go.Box(\n",
    "    y = y1,\n",
    "    name = 'The Separation',\n",
    "    marker = dict(\n",
    "        color = 'rgb(0, 128, 128)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Box(\n",
    "    y = y2,\n",
    "    name = 'The Definition',\n",
    "    marker = dict(\n",
    "        color = 'rgb(10, 140, 208)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace3 = go.Box(\n",
    "    y = y3,\n",
    "    name = 'The Human Condition',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 102, 14)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace4 = go.Box(\n",
    "    y = y4,\n",
    "    name = 'Glory Sound Prep',\n",
    "    marker = dict(\n",
    "        color = 'rgb(100, 0, 10)',\n",
    "    )\n",
    ")\n",
    "\n",
    "box_data = [trace0, trace1, trace2, trace3, trace4]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Sentiment Polarity Box Plot for Jon Bellion Albums'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = box_data, layout = layout)\n",
    "iplot(fig, filename = 'Sentiment Polarity Box Plot for Jon Bellion Albums')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(data,\n",
    "x = data.album,\n",
    "y = data.polarity.round(3),\n",
    "hover_name = 'titles',\n",
    "color = 'album'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A boxplot showcasing the toxicity range for the albums\n",
    "y0 = data.loc[data['album'] == 'translation_through_speakers']['toxicity']\n",
    "y1 = data.loc[data['album'] == 'the_separation']['toxicity']\n",
    "y2 = data.loc[data['album'] == 'the_definition']['toxicity']\n",
    "y3 = data.loc[data['album'] == 'the_human_condition']['toxicity']\n",
    "y4 = data.loc[data['album'] == 'glory_sound_prep']['toxicity']\n",
    "\n",
    "trace0 = go.Box(\n",
    "    y = y0,\n",
    "    name = 'Translation Through Speakers',\n",
    "    marker = dict(\n",
    "        color = 'rgb(214, 12, 140)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace1 = go.Box(\n",
    "    y = y1,\n",
    "    name = 'The Separation',\n",
    "    marker = dict(\n",
    "        color = 'rgb(0, 128, 128)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace2 = go.Box(\n",
    "    y = y2,\n",
    "    name = 'The Definition',\n",
    "    marker = dict(\n",
    "        color = 'rgb(10, 140, 208)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace3 = go.Box(\n",
    "    y = y3,\n",
    "    name = 'The Human Condition',\n",
    "    marker = dict(\n",
    "        color = 'rgb(12, 102, 14)',\n",
    "    )\n",
    ")\n",
    "\n",
    "trace4 = go.Box(\n",
    "    y = y4,\n",
    "    name = 'Glory Sound Prep',\n",
    "    marker = dict(\n",
    "        color = 'rgb(100, 0, 10)',\n",
    "    )\n",
    ")\n",
    "\n",
    "box_data = [trace0, trace1, trace2, trace3, trace4]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = 'Toxicity Box Plot for Jon Bellion Albums'\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = box_data, layout = layout)\n",
    "iplot(fig, filename = 'Toxicity Box Plot for Jon Bellion Albums')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting top words\n",
    "def get_top_n_words(corpus, n = None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis = 0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_words(data['processed_lyrics'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df1 = pd.DataFrame(common_words, columns = ['processed_lyrics' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the top words and their frequency\n",
    "df1.groupby('processed_lyrics').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar',\n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 20 words in lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting top number of bi-gramns\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_bigram(data['processed_lyrics'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "    \n",
    "df3 = pd.DataFrame(common_words, columns = ['processed_lyrics' , 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting bi-grams frequency\n",
    "df3.groupby('processed_lyrics').sum()['count'].sort_values(ascending=False).iplot(\n",
    "    kind='bar', \n",
    "    yTitle='Count', \n",
    "    linecolor='black', \n",
    "    title='Top 20 bigrams in lyrics')"
   ]
  },
  {
   "source": [
    "## Topic Modelling\n",
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words = 'english', max_features = 40000)\n",
    "lyrics_data = data.processed_lyrics.values\n",
    "\n",
    "print('LYRICS BEFORE VECTORIZATION: {}'.format(lyrics_data[45]))\n",
    "\n",
    "document_term_matrix = count_vectorizer.fit_transform(lyrics_data)\n",
    "\n",
    "print('LYRICS AFTER VECTORIZATION: \\n{}'.format(document_term_matrix[45]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "num_of_topics = 10\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components = num_of_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    This returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    This returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    This returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(num_of_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_lsa = find_top_n_words(10, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Loss and Misery',\n",
    "'Expressing Love & Desperation',\n",
    "'Contemplation',\n",
    "'Comfort & Accomplishments',\n",
    "'Youthfulness',\n",
    "'Bonds & Relationships',\n",
    "'Struggle & Turmoil',\n",
    "'Nurturing',\n",
    "'Contentment',\n",
    "'Dream & Fantasies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_words = find_top_n_words(5, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + topic_labels[i] for i in lsa_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(40,20))\n",
    "ax.bar(lsa_categories, lsa_counts);\n",
    "ax.set_xticks(lsa_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_title('LSA TOPIC COUNT');\n",
    "ax.set_ylabel('NUMBER OF SONGS');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne_lsa_model = TSNE(n_components = 2, perplexity = 50, learning_rate = 100, n_iter = 2000, verbose = 1, random_state = 0, angle = 0.75)\n",
    "tsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_topic_vectors(keys, two_dim_vectors):\n",
    "    '''\n",
    "    This returns a list of centroid vectors from each predicted topic category\n",
    "    '''\n",
    "    mean_topic_vectors = []\n",
    "    for t in range(num_of_topics):\n",
    "        articles_in_that_topic = []\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == t:\n",
    "                articles_in_that_topic.append(two_dim_vectors[i])    \n",
    "        \n",
    "        articles_in_that_topic = np.vstack(articles_in_that_topic)\n",
    "        mean_article_in_that_topic = np.mean(articles_in_that_topic, axis=0)\n",
    "        mean_topic_vectors.append(mean_article_in_that_topic)\n",
    "    return mean_topic_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\n",
    "colormap = colormap[:num_of_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "top_5_words_lda = find_top_n_words(5, lsa_keys, document_term_matrix, count_vectorizer)\n",
    "lsa_mean_topic_vectors = get_mean_topic_vectors(lsa_keys, tsne_lsa_vectors)\n",
    "\n",
    "plot = figure(title=\"t-SNE CLUSTERING OF {} LSA LYRICS TOPICS\".format(num_of_topics), plot_width=1200, plot_height=500)\n",
    "plot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_keys])\n",
    "\n",
    "for t in range(num_of_topics):\n",
    "    label = Label(x=lsa_mean_topic_vectors[t][0], y=lsa_mean_topic_vectors[t][1], \n",
    "                  text=topic_labels[t], text_color=colormap[t])\n",
    "    plot.add_layout(label)\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Topics'] = lsa_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_csv('final_jon_bellion_data.csv')"
   ]
  },
  {
   "source": [
    "## Search for songs based on a given word"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet', quiet = True)\n",
    "import inflect\n",
    "def find_synonym(word_entry):\n",
    "    found_synonyms = []\n",
    "    for synonym in wordnet.synsets(word_entry):\n",
    "        for lemma in synonym.lemmas():\n",
    "            found_synonyms.append(lemma.name())\n",
    "    return list(set(found_synonyms))\n",
    "\n",
    "def song_search(value, df, toggle):\n",
    "    if value == '':\n",
    "        if df.shape[0] < 25:\n",
    "            return df, df.shape[0]\n",
    "        return df[:], df.shape[0]\n",
    "    df_out = pd.DataFrame({'titles': [], 'album': [], 'date_released': [], 'lyrics': []})\n",
    "    if toggle == 'Lyrics':\n",
    "        x = []\n",
    "        for i in value.split():\n",
    "            x.extend(find_synonym(i)) # + find_synonym(p.plural(i)))\n",
    "        if x == []:\n",
    "            df_out = df.loc[df['processed_lyrics'].str.contains(value, case = False, na = False)]\n",
    "        else:\n",
    "            reg = ' | '.join(x)\n",
    "            reg = ' ' + reg\n",
    "            df_out = df.loc[df['processed_lyrics'].str.contains(reg, regex = True, case = False, na = False)]\n",
    "\n",
    "    if toggle == 'Tags':\n",
    "        x = value.split(' ')\n",
    "        reg = ' | '.join(x)\n",
    "        df_out = df.loc[df['empath_themes'].str.contains(' ' + reg, regex = True, case = False, na = False)]\n",
    "\n",
    "    if df_out.shape[0] < 25:\n",
    "        return df_out, df_out.shape[0]\n",
    "    return df_out[:], df_out.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "search_toggle = widgets.Dropdown(\n",
    "    options = ['Lyrics', 'Tags'],\n",
    "    description = 'Search:',\n",
    "    disabled = False\n",
    ")\n",
    "\n",
    "search = widgets.Text(\n",
    "    placeholder = 'Type Something',\n",
    "    description = 'Search:',\n",
    "    disabled = False\n",
    ")\n",
    "\n",
    "display(search_toggle)\n",
    "display(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "fr_sh, size = song_search(search.value, data, search_toggle.value)\n",
    "df = fr_sh[['titles', 'album']].to_html(escape = False, index = False)\n",
    "print('Number of songs about', search.value, 'is: ' + str(size))\n",
    "display(HTML(df))"
   ]
  }
 ]
}