{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loading Data and Preparing Texts for Cleaning\n",
    "--- \n",
    "This is a sample text cleaning code, I was trying my hands on a couple of things so you will a lot of code lines which can be simplified. I will upload it when it is done. However this code should work perfectly for this project."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importinig modules\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import textblob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "#Loading data\n",
    "directory_path = '/Users/qab/Desktop/Personal/NLP Projects/Context Maturity (NLP)/Data'\n",
    "tts_music_files = os.listdir(directory_path + '/translation through speakers')\n",
    "ts_music_files = os.listdir(directory_path + '/the separation')\n",
    "td_music_files = os.listdir(directory_path + '/the definition')\n",
    "thc_music_files = os.listdir(directory_path + '/the human condition')\n",
    "gsp_music_files = os.listdir(directory_path + '/glory sound prep')\n",
    "\n",
    "tts = []\n",
    "ts = []\n",
    "td = []\n",
    "thc = []\n",
    "gsp = []\n",
    "\n",
    "for i, music in enumerate(tts_music_files):\n",
    "    with open(directory_path + '/translation through speakers/' + music, 'rt') as file:\n",
    "        tts.append(file.read())\n",
    "\n",
    "for i, music in enumerate(ts_music_files):\n",
    "    with open(directory_path + '/the separation/' + music, 'rt') as file:\n",
    "        ts.append(file.read())\n",
    "\n",
    "for i, music in enumerate(td_music_files):\n",
    "    with open(directory_path + '/the definition/' + music, 'rt') as file:\n",
    "        td.append(file.read())\n",
    "\n",
    "for i, music in enumerate(thc_music_files):\n",
    "    with open(directory_path + '/the human condition/' + music, 'rt') as file:\n",
    "        thc.append(file.read())\n",
    "\n",
    "for i, music in enumerate(gsp_music_files):\n",
    "    with open(directory_path + '/glory sound prep/' + music, 'rt') as file:\n",
    "        gsp.append(file.read())\n",
    "\n",
    "tts_dictionary = dict(zip(tts_music_files, tts))\n",
    "ts_dictionary = dict(zip(ts_music_files, ts))\n",
    "td_dictionary = dict(zip(td_music_files, td))\n",
    "thc_dictionary = dict(zip(thc_music_files, thc))\n",
    "gsp_dictionary = dict(zip(gsp_music_files, gsp)) \n",
    "\n",
    "#Combining list of texts and putting it into a pandas dataframe\n",
    "def lyrics_together(lyrics_list):\n",
    "    #Takes a list of text and combines them into one large chunk of text.\n",
    "    lyrics_together = ''.join(lyrics_list)\n",
    "    return lyrics_together\n",
    "\n",
    "tts_combined = {key: [lyrics_together(value)] for (key, value) in tts_dictionary.items()}\n",
    "ts_combined = {key: [lyrics_together(value)] for (key, value) in ts_dictionary.items()}\n",
    "td_combined = {key: [lyrics_together(value)] for (key, value) in td_dictionary.items()}\n",
    "thc_combined = {key: [lyrics_together(value)] for (key, value) in thc_dictionary.items()}\n",
    "gsp_combined = {key: [lyrics_together(value)] for (key, value) in gsp_dictionary.items()}\n",
    "\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "tts = pd.DataFrame.from_dict(tts_combined).transpose()\n",
    "tts.columns = ['lyrics']\n",
    "tts = tts.sort_index()\n",
    "tts = pd.DataFrame.reset_index(tts, drop = False)\n",
    "tts.rename(columns = {'index':'titles'}, inplace = True)\n",
    "\n",
    "ts = pd.DataFrame.from_dict(ts_combined).transpose()\n",
    "ts.columns = ['lyrics']\n",
    "ts = ts.sort_index()\n",
    "ts = pd.DataFrame.reset_index(ts, drop = False)\n",
    "ts.rename(columns = {'index':'titles'}, inplace = True)\n",
    "\n",
    "td = pd.DataFrame.from_dict(td_combined).transpose()\n",
    "td.columns = ['lyrics']\n",
    "td = td.sort_index()\n",
    "td = pd.DataFrame.reset_index(td, drop = False)\n",
    "td.rename(columns = {'index':'titles'}, inplace = True)\n",
    "\n",
    "thc = pd.DataFrame.from_dict(thc_combined).transpose()\n",
    "thc.columns = ['lyrics']\n",
    "thc = thc.sort_index()\n",
    "thc = pd.DataFrame.reset_index(thc, drop = False)\n",
    "thc.rename(columns = {'index':'titles'}, inplace = True)\n",
    "\n",
    "gsp = pd.DataFrame.from_dict(gsp_combined).transpose()\n",
    "gsp.columns = ['lyrics']\n",
    "gsp = gsp.sort_index()\n",
    "gsp = pd.DataFrame.reset_index(gsp, drop = False)\n",
    "gsp.rename(columns = {'index':'titles'}, inplace = True)\n",
    "\n",
    "#Defining a function for initial text cleaning\n",
    "stop_words = stopwords.words(\"english\")\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "translation_through_speakers = pd.DataFrame(tts.lyrics)\n",
    "the_separation = pd.DataFrame(ts.lyrics)\n",
    "the_definition = pd.DataFrame(td.lyrics) \n",
    "the_human_condition = pd.DataFrame(thc.lyrics)\n",
    "glory_sound_prep = pd.DataFrame(gsp.lyrics)\n",
    "\n",
    "tts_titles = ['dead man wallking', 'dont ask cuz i dont know', 'for the dreamers', 'life', 'paper planes', 'the wonder years', 'timeless', 'waves of loneliness', 'while you count sheep', 'wutup snow']\n",
    "\n",
    "ts_titles = ['2 rocking chairs', 'eyes to the sky', 'halloween', 'jim morrison', 'kingdom come', 'newyorksoul', 'one more time', 'superman, the gift and the curse', 'to my future wife...', 'ungrateful eyes', 'when the lions come']\n",
    "\n",
    "td_titles = ['a haunted house', 'an immigrant', 'carry your throne', 'human', 'jungle', 'luxury', 'munny right', 'ooh', 'pre-occupied', 'run wild', 'simple and sweet']\n",
    "\n",
    "thc_titles = [\"80's films\", \"all time low\", \"fashion\", \"guillotine\", \"hand of god (outro)\", \"he is the same\", \"irobot\", \"maybe idk\", \"morning in america\", \"new york soul (part ii)\", \"overwhelming\", \"the good in me\", \"weight of the world\", \"woke the fuck up\"]\n",
    "\n",
    "gsp_titles = ['adult swim', 'blu', 'cautionary tales', 'conversations with my wife', 'couples retreat', 'jt', \"let's begin\", \"mah's joint\", 'stupid deep', 'the internet']\n",
    "\n",
    "translation_through_speakers['titles'] = tts_titles\n",
    "the_definition['titles'] = td_titles\n",
    "the_separation['titles'] = ts_titles\n",
    "the_human_condition['titles'] = thc_titles\n",
    "glory_sound_prep['titles'] = gsp_titles\n",
    "\n",
    "translation_through_speakers['date_released'] = 'February 20, 2013'\n",
    "the_separation['date_released'] = 'December 10, 2013'\n",
    "the_definition['date_released'] = 'September 23, 2014'\n",
    "the_human_condition['date_released'] = 'June 20, 2016'\n",
    "glory_sound_prep['date_released'] = 'November 9, 2018'\n",
    "\n",
    "translation_through_speakers['album'] = 'translation_through_speakers'\n",
    "the_separation['album'] = 'the_separation'\n",
    "the_definition['album'] = 'the_definition'\n",
    "the_human_condition['album'] = 'the_human_condition'\n",
    "glory_sound_prep['album'] = 'glory_sound_prep'\n",
    "\n",
    "jon_bellion = pd.concat([translation_through_speakers, the_separation, the_definition, the_human_condition, glory_sound_prep], axis=0)\n",
    "jon_bellion = pd.DataFrame.reset_index(jon_bellion, drop = True)\n",
    "\n",
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\", \"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\", \"gonna\": \"going to\", \"Gonna\": \"going to\", \"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\", \"he's\": \"he is\",\n",
    "                     \"he'd've\": \"he would have\",\"He'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"i would\", \"I'd've\": \"i would have\",\"I'll\": \"i will\",\n",
    "                     \"I'll've\": \"i will have\",\"I'm\": \"i am\",\"I've\": \"i have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"She'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"They'll\": \"they will\", \"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"They're\": \"they are\", \"they're\": \"they are\", \"They've\": \"they have\", \"hey've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"tryna\": \"trying to\", \"Tryna\": \"trying to\", \"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text, contractions_dict = contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "def final_clean(lyrics):\n",
    "    lyrics = lyrics.lower()\n",
    "    lyrics = re.sub('\\n', ' ', lyrics)\n",
    "    lyrics = re.sub('\\[.*?\\]', '', lyrics)\n",
    "    lyrics = ' '.join(wordnet.lemmatize(word, 'v') for word  in lyrics.split())\n",
    "    lyrics = ' '.join([word for word in lyrics.split(' ') if word not in stop_words])\n",
    "    lyrics = re.sub('[''\"\"’—...]', '', lyrics)\n",
    "    lyrics = re.sub('\\w*\\d\\w*', '', lyrics)\n",
    "    lyrics = re.sub('[%s]' % re.escape(string.punctuation), '', lyrics)\n",
    "    return lyrics\n",
    "cleaning = lambda song: final_clean(song)\n",
    "\n",
    "# Expanding contractions in the lyrics\n",
    "jon_bellion['processed_lyrics'] = jon_bellion['lyrics'].apply(lambda x: expand_contractions(x))\n",
    "\n",
    "# Applying final cleaning to the lyrics\n",
    "jon_bellion['processed_lyrics'] = jon_bellion['processed_lyrics'].apply(final_clean)\n",
    "\n",
    "not_needed = ['ahhh', 'yeah', 'bum', 'la', 'lalalalala', 'da', 'ba', 'oh', 'ohh', 'ooh', 'mama', 'nana', 'yah', 'uh', 'ew', 'dum', 'ho', 'ya', 'nay', 'nan', 'yo', 'nanana', 'uhuh', 'badem', 'buhda', 'dadada', 'dadum', 'gogo', 't', 'im', 'tttell', 'i', 'llie']\n",
    "words_to_remove = r'\\b(?:{})\\b'.format('|'.join(not_needed))\n",
    "\n",
    "jon_bellion['processed_lyrics'] = jon_bellion['processed_lyrics'].str.replace(words_to_remove, '')\n",
    "\n",
    "jon_bellion = jon_bellion[['titles', 'album', 'date_released', 'lyrics', 'processed_lyrics']]\n",
    "\n",
    "jon_bellion.to_csv(directory_path + '/' + 'jon_bellion.csv', index = False)"
   ]
  }
 ]
}